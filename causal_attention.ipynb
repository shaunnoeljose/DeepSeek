{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0648b4da",
   "metadata": {},
   "source": [
    "#### Casual Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee51456",
   "metadata": {},
   "source": [
    "We need to mask the upper triangle of the attention scores/ attention weights to not allow the tokens see the context for the future tokens. There are 2 ways to do it:\n",
    "\n",
    "* Masking the attention weights after the softmax is applied. We mask the uppper triangle of the attention weights matrix with 0 and then peform another round of normalization.\n",
    "* The 2nd method is to mask the uper triangle of the attention scores matrix and then apply the soft-max nornmalization technique.\n",
    "\n",
    "The 1st method is more efficient as it avoids multiple normalisation and uses less computation.\n",
    "\n",
    "**Efficinent method**\n",
    "\n",
    "Attention scores --> Upper triangle -ve infinity mask --> softmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d2528e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3dace7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "    [[0.43,0.15,0.89], # Your\n",
    "    [0.55, 0.87, 0.66], # journey\n",
    "    [0.57, 0.85, 0.64], # starts\n",
    "    [0.22, 0.58, 0.33], # with\n",
    "    [0.77, 0.25, 0.10], # one\n",
    "    [0.05, 0.80, 0.55]] # step\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d784da9",
   "metadata": {},
   "source": [
    "Defining elements\n",
    "\n",
    "* A: The second input element\n",
    "* B: Input embedding size, d_in=3\n",
    "* C: Output embedding size, d_out=2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e73fccb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape is 3\n",
      "Output shape is 2\n",
      "Query weights:Parameter containing:\n",
      "tensor([[0.8943, 0.8260],\n",
      "        [0.4370, 0.6476],\n",
      "        [0.5942, 0.3844]])\n",
      " \n",
      "Key weights:Parameter containing:\n",
      "tensor([[0.1322, 0.7668],\n",
      "        [0.7084, 0.6391],\n",
      "        [0.6801, 0.3480]])\n",
      " \n",
      "Value weights:Parameter containing:\n",
      "tensor([[0.7519, 0.4936],\n",
      "        [0.6580, 0.5615],\n",
      "        [0.2156, 0.6007]])\n",
      " \n",
      "The tensor value for the second token is tensor([0.5500, 0.8700, 0.6600])\n"
     ]
    }
   ],
   "source": [
    "d_in = inputs.shape[1]\n",
    "print(f\"Input shape is {d_in}\")\n",
    "d_out = 2\n",
    "print(f\"Output shape is {d_out}\")\n",
    "\n",
    "# Intialising the weight matrices\n",
    "\n",
    "\"\"\"\n",
    "requires_grad is set to False to reduce clutter. But if we were to use the weight matrices for training we would set up to be equal to True.\n",
    "So that it updates the amtricews during model training.\n",
    "\"\"\"\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad = False) \n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad = False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad = False)\n",
    "\n",
    "print(f\"Query weights:{W_query}\")\n",
    "print(\" \")\n",
    "print(f\"Key weights:{W_key}\")\n",
    "print(\" \")\n",
    "print(f\"Value weights:{W_value}\")\n",
    "print(\" \")\n",
    "\n",
    "\"\"\"\n",
    "For GPT like models the input and the output dimensions are usually the same. But for demostration we are using different dimensions\n",
    "\"\"\"\n",
    "x_2 = inputs[1]\n",
    "print(f\"The tensor value for the second token is {x_2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "885da3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The query vector for the 2nd token is tensor([1.2643, 1.2714])\n",
      "The key vector for the 2nd token is tensor([1.1379, 1.2074])\n",
      "The value vector for the 2nd token is tensor([1.1282, 1.1564])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We get a 1x2 dimensional query, key and value vector. Even though our temporary goal is to only compute the one context vector z(2).\n",
    "We still require key and value vectors for all the input. As this is required for the calculation of the attention weights\n",
    "with respect to the query q(2).\n",
    "\"\"\"\n",
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "\n",
    "print(f\"The query vector for the 2nd token is {query_2}\")\n",
    "print(f\"The key vector for the 2nd token is {key_2}\")\n",
    "print(f\"The value vector for the 2nd token is {value_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4568a107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of queries matrix: torch.Size([6, 2])\n",
      "Shape of keys matrix: torch.Size([6, 2])\n",
      "Shape of values matrix: torch.Size([6, 2])\n",
      " \n",
      "Attention score between the 2nd token and the 2nd token: 2.9736814498901367\n",
      " \n",
      "Attention score for the entire 2nd token: tensor([1.9062, 2.9737, 2.9363, 1.6717, 1.4366, 2.1398])\n",
      " \n",
      "Entire attention score matrix: tensor([[1.3363, 2.0731, 2.0450, 1.1701, 0.9632, 1.5161],\n",
      "        [1.9062, 2.9737, 2.9363, 1.6717, 1.4366, 2.1398],\n",
      "        [1.9011, 2.9656, 2.9283, 1.6672, 1.4325, 2.1342],\n",
      "        [0.9997, 1.5615, 1.5423, 0.8770, 0.7613, 1.1193],\n",
      "        [1.2737, 1.9853, 1.9601, 1.1167, 0.9539, 1.4319],\n",
      "        [1.1209, 1.7513, 1.7297, 0.9834, 0.8552, 1.2544]])\n"
     ]
    }
   ],
   "source": [
    "# Computing the query, key and value vectors\n",
    "\n",
    "queries = inputs @ W_query\n",
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "\n",
    "# We have projected the 6 input tokens from a 3D space onto a 2D embedding space.\n",
    "print(f\"Shape of queries matrix: {queries.shape}\")\n",
    "print(f\"Shape of keys matrix: {keys.shape}\")\n",
    "print(f\"Shape of values matrix: {values.shape}\")\n",
    "print(\" \")\n",
    "\n",
    "# Computing the attention score for the 2nd token\n",
    "keys_2 = keys[1]\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(f\"Attention score between the 2nd token and the 2nd token: {attn_score_22}\")\n",
    "print(\" \")\n",
    "\n",
    "# Generalising the computation to get all attention scores by matrix multiplication for the 2nd token\n",
    "attn_score_2 = query_2 @ keys.T #all attention scores for the 2nd token(query)\n",
    "print(f\"Attention score for the entire 2nd token: {attn_score_2}\")\n",
    "print(\" \")\n",
    "\n",
    "# Entire attention score matrix\n",
    "attn_score = queries @ keys.T\n",
    "print(f\"Entire attention score matrix: {attn_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9122be93",
   "metadata": {},
   "source": [
    "The next step is to calculate the attention weights by scaling the attention scores and performing a softmax operation. For causal attention we need to mask the upper triangle with -ve infinity. We can resue the class SelfAttention_v2 from multi_head_attention.ipynb notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76835447",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Yopur\n",
    "     [0.55, 0.87, 0.66], # journey\n",
    "     [0.57, 0.85, 0.64], # starts\n",
    "     [0.22, 0.58, 0.33], # with\n",
    "     [0.77, 0.25, 0.10], # one\n",
    "     [0.05, 0.80, 0.55]] # step\n",
    ")\n",
    "\n",
    "d_in = 3\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f65af343",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x) \n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weight = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim = -1)\n",
    "\n",
    "        context_vec = attn_weight @ values\n",
    "        return context_vec\n",
    "        \n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43011d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1600, 0.1734, 0.1732, 0.1638, 0.1629, 0.1666],\n",
      "        [0.1617, 0.1700, 0.1702, 0.1648, 0.1695, 0.1638],\n",
      "        [0.1620, 0.1697, 0.1699, 0.1649, 0.1697, 0.1638],\n",
      "        [0.1638, 0.1686, 0.1686, 0.1656, 0.1683, 0.1651],\n",
      "        [0.1681, 0.1634, 0.1637, 0.1675, 0.1736, 0.1636],\n",
      "        [0.1611, 0.1717, 0.1716, 0.1644, 0.1658, 0.1655]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim = 1 )\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bb1579f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      " \n",
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      " \n",
      "tensor([[0.1600, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1617, 0.1700, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1620, 0.1697, 0.1699, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1638, 0.1686, 0.1686, 0.1656, 0.0000, 0.0000],\n",
      "        [0.1681, 0.1634, 0.1637, 0.1675, 0.1736, 0.0000],\n",
      "        [0.1611, 0.1717, 0.1716, 0.1644, 0.1658, 0.1655]],\n",
      "       grad_fn=<MulBackward0>)\n",
      " \n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4875, 0.5125, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3230, 0.3384, 0.3387, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2457, 0.2528, 0.2530, 0.2484, 0.0000, 0.0000],\n",
      "        [0.2010, 0.1953, 0.1958, 0.2003, 0.2076, 0.0000],\n",
      "        [0.1611, 0.1717, 0.1716, 0.1644, 0.1658, 0.1655]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 1st Method --> Updating attention weights above teh diagonal to zero and then normalising \n",
    "\n",
    "# We can use PyTorch tril function to create a mask where the values above the diagonal are zero\n",
    "context_length = attn_scores.shape[0]\n",
    "print(torch.ones(context_length, context_length))\n",
    "print(\" \")\n",
    "\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length)) # Masking upper diagonal with zero\n",
    "print(mask_simple)\n",
    "print(\" \")\n",
    "\n",
    "# Multiplying the masked matrix with the attention weights to zsero out the upper diagonal values.\n",
    "masked_simple = attn_weights * mask_simple\n",
    "print(masked_simple)\n",
    "print(\" \")\n",
    "\n",
    "# The elements above the diagonal are zeroed out but needs to be normalised\n",
    "row_sums = masked_simple.sum(dim =1, keepdim = True)\n",
    "masked_simple_norm = masked_simple/row_sums\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bc46f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0817,  0.1954,  0.1936,  0.1148,  0.1072,  0.1386],\n",
      "        [ 0.0263,  0.0971,  0.0983,  0.0533,  0.0927,  0.0449],\n",
      "        [ 0.0223,  0.0882,  0.0896,  0.0481,  0.0886,  0.0382],\n",
      "        [ 0.0149,  0.0551,  0.0559,  0.0303,  0.0528,  0.0254],\n",
      "        [-0.0556, -0.0959, -0.0928, -0.0604, -0.0101, -0.0940],\n",
      "        [ 0.0545,  0.1446,  0.1441,  0.0834,  0.0959,  0.0925]],\n",
      "       grad_fn=<MmBackward0>)\n",
      " \n",
      "tensor([[ 0.0817,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.0263,  0.0971,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.0223,  0.0882,  0.0896,    -inf,    -inf,    -inf],\n",
      "        [ 0.0149,  0.0551,  0.0559,  0.0303,    -inf,    -inf],\n",
      "        [-0.0556, -0.0959, -0.0928, -0.0604, -0.0101,    -inf],\n",
      "        [ 0.0545,  0.1446,  0.1441,  0.0834,  0.0959,  0.0925]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      " \n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4875, 0.5125, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3230, 0.3384, 0.3387, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2457, 0.2528, 0.2530, 0.2484, 0.0000, 0.0000],\n",
      "        [0.2010, 0.1953, 0.1958, 0.2003, 0.2076, 0.0000],\n",
      "        [0.1611, 0.1717, 0.1716, 0.1644, 0.1658, 0.1655]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 2nd Method --> Updating attention scores above the diagonal to -ve infinity and then applying scaling normalising(softmax) to get attention scores\n",
    "print(attn_scores)  \n",
    "print(\" \")\n",
    "\n",
    "mask = torch.triu(torch.ones(context_length,context_length), diagonal = 1 )\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)\n",
    "print(\" \")\n",
    "\n",
    "# applying softmax to the masked matrix, changes the -ve infinity to 0s and sum of every row = 1\n",
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim = 1)\n",
    "print(attn_weights) \n",
    "\n",
    "# Both the methods give us the same answer. But the 2nd method is more efficient than the 1st one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cde3f15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
