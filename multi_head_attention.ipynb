{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "361b9e36",
   "metadata": {},
   "source": [
    "## Multi Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dabc91a",
   "metadata": {},
   "source": [
    "### Why scaling is important before softmax\n",
    "\n",
    "* We scale by sqrt(key dimension) so that the variance does not blow up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb422d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab96b6a",
   "metadata": {},
   "source": [
    "In attention mechanism, when the dot product between query and key vectors becomes too large (similar to the example of multipl;ying by 8), the attention scores can become very large. This results in a very sharp softmax distribution, making the model overly confident in one particular \"key\". Such sharp distributions can make learning unstable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cfecd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax without scaling: tensor([0.2359, 0.1748, 0.2881, 0.1431, 0.1581])\n",
      "Softmax with scaling: tensor([0.1639, 0.0149, 0.8116, 0.0030, 0.0067])\n"
     ]
    }
   ],
   "source": [
    "# define the tensor\n",
    "tensor = torch.tensor([0.1, -0.2, 0.3, -0.4, -0.3])\n",
    "\n",
    "# apply softmax without scaling\n",
    "softmax_result = torch.softmax(tensor, dim =-1)\n",
    "print(\"Softmax without scaling:\", softmax_result)\n",
    "\n",
    "# apply softmax with scaling\n",
    "scaled_tensor = tensor*8\n",
    "softmax_scaled_result = torch.softmax(scaled_tensor, dim = -1)\n",
    "print(\"Softmax with scaling:\", softmax_scaled_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9700b1",
   "metadata": {},
   "source": [
    "* This is not good as the weights are not proportionally distributed. This can be an issue with the attention mechanism.\n",
    "* **Thus, we need to have normalisation before applying softmax on the tensor to have better proportion of weights to add upto 1.**\n",
    "* **Normalization** is done by dividing the dot product of query matrix and key matrix (transpose). We get the attention scores.\n",
    "* Then we convert the **attention scores** to **attention weights**\n",
    "* We need to make the variance of the dot product stable\n",
    "\n",
    "We divide by the sqrt(dimension) to have more stability while learning and also have a stable variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edb17f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance before scaling (dim=5): 4.971412062399333\n",
      "variance after scaling (dim=5): 0.9942824124798666\n",
      "Variance before scaling (dim=100): 97.12069181019781\n",
      "variance after scaling (dim=100): 0.9712069181019781\n"
     ]
    }
   ],
   "source": [
    "# Function to compute variance before and after scaling\n",
    "\n",
    "def compute_variance(dim, trials = 1000):\n",
    "    dot_products = []\n",
    "    scaled_dot_products = []\n",
    "\n",
    "    # Generate multiple random vectors and compute dot products\n",
    "    for _ in range(trials):\n",
    "        q = np.random.randn(dim)\n",
    "        k = np.random.randn(dim)\n",
    "\n",
    "        # getting dot product\n",
    "        dot_product = np.dot(q,k)\n",
    "        dot_products.append(dot_product)\n",
    "\n",
    "        # scale the dot product by sqrt\n",
    "        scaled_dot_product = dot_product/np.sqrt(dim)\n",
    "        scaled_dot_products.append(scaled_dot_product)\n",
    "    \n",
    "    variance_before_scaling = np.var(dot_products)\n",
    "    variance_after_scaling = np.var(scaled_dot_products)\n",
    "\n",
    "    return variance_before_scaling, variance_after_scaling\n",
    "\n",
    "variance_before_5, variance_after_5 = compute_variance(5)\n",
    "print(f\"Variance before scaling (dim=5): {variance_before_5}\")\n",
    "print(f\"variance after scaling (dim=5): {variance_after_5}\")\n",
    "\n",
    "variance_before_100, variance_after_100 = compute_variance(100)\n",
    "print(f\"Variance before scaling (dim=100): {variance_before_100}\")\n",
    "print(f\"variance after scaling (dim=100): {variance_after_100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe54fec4",
   "metadata": {},
   "source": [
    "#### Implementing a compact self attention python class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d784d671",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89],\n",
    "     [0.55, 0.87, 0.66],\n",
    "     [0.57, 0.85, 0.64],\n",
    "     [0.22, 0.58, 0.33],\n",
    "     [0.77, 0.25, 0.10],\n",
    "     [0.05, 0.80, 0.55]]\n",
    ")\n",
    "\n",
    "d_in = 3\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4eaea08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class SelfAttention_v1(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = x @ self.W_query\n",
    "        keys = x @ self.W_key\n",
    "        values = x @ self.W_value\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1] **0.5, dim = -1\n",
    "        )\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "    \n",
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c567ff42",
   "metadata": {},
   "source": [
    "Since the input contains 6 vectors we get a matrix storing the 6 context vectors. To improve the SelfAttention_v1 implementation, we can use PyTorch's nn.Linear layers. This effectively performs the matrix multiplication when the bias units are disabled.\n",
    "\n",
    "nn.Linear layer has an optimised weight intialisation scheme, contributing to more stable and effective model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973a7aa5",
   "metadata": {},
   "source": [
    "#### Adding a self attention layer and looking at the flow of the matrix multiplication between query, key and value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3994bd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1370,  0.2535],\n",
      "        [-0.1289,  0.2542],\n",
      "        [-0.1293,  0.2541],\n",
      "        [-0.1278,  0.2542],\n",
      "        [-0.1365,  0.2535],\n",
      "        [-0.1240,  0.2545]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x) \n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weight = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim = -1)\n",
    "\n",
    "        context_vec = attn_weight @ values\n",
    "        return context_vec\n",
    "        \n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "\n",
    "print(sa_v2(inputs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa41d6f",
   "metadata": {},
   "source": [
    "Both SelfAttention_v1 and SelfAttention_v2 give different outputs as they use different intial weights for the weight matrices since nn.Linear uses a more sophisticated weight intialization scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa321563",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
